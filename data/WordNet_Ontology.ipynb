{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sammcmanagan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject   relation   object\n",
      "0     able  attribute  ability\n",
      "1     able  attribute  ability\n",
      "2   unable  attribute  ability\n",
      "3  abaxial    synonym   dorsal\n",
      "4  adaxial    synonym  ventral\n",
      "(201484, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store relationships\n",
    "data = []\n",
    "\n",
    "# Define a function to add relationships to the data list\n",
    "def add_relationships(subject_synset, relation, objects):\n",
    "    subject_name = subject_synset.lemma_names()[0]  # Extract human-readable name of the subject\n",
    "    for obj in objects:\n",
    "        data.append({\n",
    "            \"subject\": subject_name,\n",
    "            \"relation\": relation,\n",
    "            \"object\": obj.lemma_names()[0],  # Extract human-readable name of the object\n",
    "        })\n",
    "\n",
    "# Define a function to add synonym relationships\n",
    "def add_synonym_relationships(synset):\n",
    "    subject_name = synset.lemma_names()[0]  # Use the first lemma name as the subject\n",
    "    # Iterate over all the lemmas in the synset\n",
    "    for lemma in synset.lemmas():\n",
    "        # Add synonym relationships to the data list\n",
    "        if lemma.name() != subject_name:  # Avoid self-referencing\n",
    "            data.append({\n",
    "                \"subject\": subject_name,\n",
    "                \"relation\": \"synonym\",\n",
    "                \"object\": lemma.name(),\n",
    "            })\n",
    "\n",
    "# Iterate over all synsets in WordNet\n",
    "for synset in wn.all_synsets():\n",
    "    subject = synset\n",
    "    \n",
    "    # Add synonyms\n",
    "    add_synonym_relationships(synset)\n",
    "    \n",
    "    # Extract hypernyms\n",
    "    add_relationships(synset, \"hypernym\", synset.hypernyms())\n",
    "    #add_relationships(synset, \"root_hypernym\", synset.root_hypernyms())\n",
    "    \n",
    "    # Extract hyponyms\n",
    "    #add_relationships(synset, \"hyponym\", synset.hyponyms())\n",
    "    \n",
    "    # Extract meronyms (part meronyms and substance meronyms)\n",
    "    add_relationships(synset, \"part_meronym\", synset.part_meronyms())\n",
    "    add_relationships(synset, \"member_meronym\", synset.member_meronyms())\n",
    "    \n",
    "    # Extract holonyms (part holonyms and substance holonyms)\n",
    "    #add_relationships(synset, \"part_holonym\", synset.part_holonyms())\n",
    "    #add_relationships(synset, \"member_holonym\", synset.member_holonyms())\n",
    "    \n",
    "    # Extract entailments (for verbs)\n",
    "    add_relationships(synset, \"entailment\", synset.entailments())\n",
    "    \n",
    "    # Extract attributes\n",
    "    add_relationships(synset, \"attribute\", synset.attributes())\n",
    "    \n",
    "\n",
    "    \n",
    "    #coordinate_terms = get_coordinate_terms(synset)\n",
    "    #add_relationships(subject, \"coordinate_term\", coordinate_terms)\n",
    "\n",
    "\n",
    "# Convert the list of relationships to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display a preview of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280531</th>\n",
       "      <td>blink</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179863</th>\n",
       "      <td>United_States</td>\n",
       "      <td>part_meronym</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8008</th>\n",
       "      <td>earlyish</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>earlyish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132816</th>\n",
       "      <td>reappraisal</td>\n",
       "      <td>synonym</td>\n",
       "      <td>reassessment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110254</th>\n",
       "      <td>straw</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>tube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105469</th>\n",
       "      <td>rack</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146306</th>\n",
       "      <td>badinage</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144123</th>\n",
       "      <td>lettercard</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>postcard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316756</th>\n",
       "      <td>condescend</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217901</th>\n",
       "      <td>Florida_yew</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>yew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288287</th>\n",
       "      <td>salt</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>spice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34553</th>\n",
       "      <td>gladly</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>gladly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124882</th>\n",
       "      <td>esophageal_smear</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>alimentary_tract_smear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304241</th>\n",
       "      <td>commend</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>portray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224942</th>\n",
       "      <td>silverrod</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101104</th>\n",
       "      <td>mitomycin</td>\n",
       "      <td>synonym</td>\n",
       "      <td>Mutamycin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141875</th>\n",
       "      <td>library_card</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39931</th>\n",
       "      <td>push</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>minimized</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>minimized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129980</th>\n",
       "      <td>hand</td>\n",
       "      <td>part_meronym</td>\n",
       "      <td>palm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 subject       relation                  object\n",
       "280531             blink  root_hypernym                     act\n",
       "179863     United_States   part_meronym           Massachusetts\n",
       "8008            earlyish  root_hypernym                earlyish\n",
       "132816       reappraisal        synonym            reassessment\n",
       "110254             straw       hypernym                    tube\n",
       "105469              rack       hypernym               framework\n",
       "146306          badinage  root_hypernym                  entity\n",
       "144123        lettercard       hypernym                postcard\n",
       "316756        condescend  root_hypernym                     act\n",
       "217901       Florida_yew       hypernym                     yew\n",
       "288287              salt       hypernym                   spice\n",
       "34553             gladly  root_hypernym                  gladly\n",
       "124882  esophageal_smear       hypernym  alimentary_tract_smear\n",
       "304241           commend       hypernym                 portray\n",
       "224942         silverrod  root_hypernym                  entity\n",
       "101104         mitomycin        synonym               Mutamycin\n",
       "141875      library_card  root_hypernym                  entity\n",
       "39931               push       hypernym                progress\n",
       "8632           minimized  root_hypernym               minimized\n",
       "129980              hand   part_meronym                    palm"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing region descriptions: 100%|██████████| 108077/108077 [00:02<00:00, 49937.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering WordNet DataFrame...\n",
      "Filtered DataFrame saved to filtered_wordnet.csv\n",
      "4297502\n",
      "21907303\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text.lower()).split()\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to normalize text (assuming it is defined elsewhere)\n",
    "def normalize_text(text):\n",
    "    # Add text normalization logic here (e.g., lowercasing, punctuation removal, etc.)\n",
    "    return text.split()  # Simple placeholder for text normalization\n",
    "\n",
    "def load_region_descriptions(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        region_descriptions = json.load(f)\n",
    "    \n",
    "    # Initialize variables to store words and count the number of phrases\n",
    "    all_words = set()\n",
    "    word_count = 0\n",
    "    phrase_count = 0\n",
    "    \n",
    "    # Iterate over region descriptions and count phrases\n",
    "    for entry in tqdm(region_descriptions, desc=\"Processing region descriptions\"):\n",
    "        for region in entry['regions']:\n",
    "            words = normalize_text(region['phrase'])\n",
    "            all_words.update(words)\n",
    "            word_count += len(words)\n",
    "            phrase_count += 1  # Increment the phrase count\n",
    "    \n",
    "    return all_words, phrase_count, word_count\n",
    "\n",
    "# Step 2: Filter WordNet DataFrame\n",
    "def filter_wordnet_df(wordnet_df, valid_words):\n",
    "    # Check if either subject or object is in the valid words set\n",
    "    return wordnet_df[\n",
    "        (wordnet_df['subject'].apply(lambda x: x in valid_words)) |\n",
    "        (wordnet_df['object'].apply(lambda x: x in valid_words))\n",
    "    ]\n",
    "\n",
    "# Main function\n",
    "def main(wordnet_df, region_descriptions_path, output_path):\n",
    "    \n",
    "    # Load the region descriptions\n",
    "    valid_words, n_phrases, word_count = load_region_descriptions(region_descriptions_path)\n",
    "    \n",
    "    # Filter the WordNet DataFrame\n",
    "    print(\"Filtering WordNet DataFrame...\")\n",
    "    filtered_df = filter_wordnet_df(wordnet_df, valid_words)\n",
    "    \n",
    "    # Save the filtered DataFrame\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "    print(f\"Filtered DataFrame saved to {output_path}\")\n",
    "    \n",
    "    return filtered_df, n_phrases, word_count\n",
    "\n",
    "# Paths\n",
    "region_descriptions_path = \"/Users/sammcmanagan/Downloads/region_descriptions.json\"\n",
    "output_path = \"filtered_wordnet.csv\"\n",
    "\n",
    "# Run the script\n",
    "filtereed_df, n_phrases, word_count = main(df, region_descriptions_path, output_path)\n",
    "\n",
    "print(n_phrases)\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>unattributable</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>unattributable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>unattributable</td>\n",
       "      <td>attribute</td>\n",
       "      <td>attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>attributive</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>attributive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>attributive_genitive</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>attributive_genitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>predicative</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>predicative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>pregnant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>big</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>big</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>nonpregnant</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>nonpregnant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>audible</td>\n",
       "      <td>root_hypernym</td>\n",
       "      <td>audible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>audible</td>\n",
       "      <td>attribute</td>\n",
       "      <td>audibility</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   subject       relation                object\n",
       "1000        unattributable  root_hypernym        unattributable\n",
       "1001        unattributable      attribute           attribution\n",
       "1002           attributive  root_hypernym           attributive\n",
       "1003  attributive_genitive  root_hypernym  attributive_genitive\n",
       "1004           predicative  root_hypernym           predicative\n",
       "1005              pregnant  root_hypernym              pregnant\n",
       "1006                   big  root_hypernym                   big\n",
       "1007           nonpregnant  root_hypernym           nonpregnant\n",
       "1008               audible  root_hypernym               audible\n",
       "1009               audible      attribute            audibility"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1000:].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201484\n",
      "97342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(df))\n",
    "print(len(filtereed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173441</th>\n",
       "      <td>fresh_water</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87919</th>\n",
       "      <td>submission</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>fresh</td>\n",
       "      <td>synonym</td>\n",
       "      <td>unused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48027</th>\n",
       "      <td>barrel</td>\n",
       "      <td>part_meronym</td>\n",
       "      <td>breech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135565</th>\n",
       "      <td>Beta</td>\n",
       "      <td>synonym</td>\n",
       "      <td>genus_Beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56056</th>\n",
       "      <td>home</td>\n",
       "      <td>synonym</td>\n",
       "      <td>rest_home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69643</th>\n",
       "      <td>savoir-faire</td>\n",
       "      <td>synonym</td>\n",
       "      <td>address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185950</th>\n",
       "      <td>cover</td>\n",
       "      <td>synonym</td>\n",
       "      <td>plow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126354</th>\n",
       "      <td>Charles</td>\n",
       "      <td>synonym</td>\n",
       "      <td>Jacques_Alexandre_Cesar_Charles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112150</th>\n",
       "      <td>liman</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>lagoon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subject      relation                           object\n",
       "173441   fresh_water      hypernym                            water\n",
       "87919     submission      hypernym                        agreement\n",
       "1913           fresh       synonym                           unused\n",
       "48027         barrel  part_meronym                           breech\n",
       "135565          Beta       synonym                       genus_Beta\n",
       "56056           home       synonym                        rest_home\n",
       "69643   savoir-faire       synonym                          address\n",
       "185950         cover       synonym                             plow\n",
       "126354       Charles       synonym  Jacques_Alexandre_Cesar_Charles\n",
       "112150         liman      hypernym                           lagoon"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtereed_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97101\n"
     ]
    }
   ],
   "source": [
    "df_filtered = filtereed_df[filtereed_df['subject'] != filtereed_df['object']]\n",
    "print(len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid entities (>= 300 occurrences):\n",
      "['watermelon', 'painting', 'rail', 'goggles', 'laptops', 'container', 'pump', 'handled', 'traveling', 'left', 'decoration', 'suitcase', 'coach', 'us', 'strawberry', 'panda', 'rimmed', 'power', 'toothbrushes', 'pizzas']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_entities_from_phrase(phrase):\n",
    "    \"\"\"\n",
    "    This function extracts entities from the given phrase.\n",
    "    We'll use a simple heuristic approach by splitting the phrase into words.\n",
    "    In a more advanced setup, you could use NLP tools like spaCy for POS tagging to identify proper nouns (nouns representing entities).\n",
    "    \"\"\"\n",
    "    # Lowercasing and basic tokenization to extract potential entities (nouns, etc.)\n",
    "    phrase = phrase.lower()  # Normalize the text\n",
    "    words = re.findall(r'\\b\\w+\\b', phrase)  # Tokenize into words\n",
    "    \n",
    "    # Example of filtering out common stopwords; this can be improved with more sophisticated NLP methods.\n",
    "    stopwords = set(['is', 'the', 'on', 'in', 'a', 'and', 'of', 'to', 'for', 'be'])\n",
    "    entities = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Load region descriptions JSON (replace with your actual path)\n",
    "with open('/Users/sammcmanagan/Downloads/region_descriptions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract entities from each phrase and count their frequencies\n",
    "entity_counter = Counter()\n",
    "for region_data in data:\n",
    "    for region in region_data['regions']:\n",
    "        entities = extract_entities_from_phrase(region['phrase'])\n",
    "        entity_counter.update(entities)\n",
    "\n",
    "# Get the most common entities\n",
    "min_frequency = 300  # Set a frequency threshold (you can adjust this)\n",
    "valid_entities = {entity for entity, count in entity_counter.items() if count >= min_frequency}\n",
    "\n",
    "# Output some example valid entities\n",
    "print(f\"Valid entities (>= {min_frequency} occurrences):\")\n",
    "print(list(valid_entities)[:20])  # Print first 20 valid entities as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3626, 3)\n",
      "    subject relation   object\n",
      "98     away  synonym  outside\n",
      "236   plain  synonym     bare\n",
      "237   plain  synonym    spare\n",
      "414    full  synonym     good\n",
      "416    wide  synonym     full\n"
     ]
    }
   ],
   "source": [
    "def filter_by_visual_genome_frequency(wordnet_df, valid_entities):\n",
    "    # Filter rows where the subject or object is in the valid_entities set\n",
    "    return wordnet_df[\n",
    "        wordnet_df['subject'].isin(valid_entities) & \n",
    "        wordnet_df['object'].isin(valid_entities)\n",
    "    ]\n",
    "\n",
    "# Apply filtering to the WordNet DataFrame (assuming you have it as `wordnet_df`)\n",
    "final_df = filter_by_visual_genome_frequency(df_filtered, valid_entities)\n",
    "\n",
    "# Show the filtered DataFrame size and first few rows\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67270</th>\n",
       "      <td>yard</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>enclosure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>small</td>\n",
       "      <td>attribute</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195533</th>\n",
       "      <td>dock</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>enter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55748</th>\n",
       "      <td>hat</td>\n",
       "      <td>part_meronym</td>\n",
       "      <td>crown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85451</th>\n",
       "      <td>minute</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171819</th>\n",
       "      <td>exhaust</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86658</th>\n",
       "      <td>material</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87340</th>\n",
       "      <td>decoration</td>\n",
       "      <td>synonym</td>\n",
       "      <td>ribbon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53484</th>\n",
       "      <td>enclosure</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65599</th>\n",
       "      <td>tongs</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53967</th>\n",
       "      <td>fighter</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194365</th>\n",
       "      <td>fall</td>\n",
       "      <td>hypernym</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subject      relation       object\n",
       "67270         yard      hypernym    enclosure\n",
       "6103         small     attribute         size\n",
       "195533        dock      hypernym        enter\n",
       "55748          hat  part_meronym        crown\n",
       "85451       minute      hypernym         note\n",
       "171819     exhaust      hypernym          gas\n",
       "86658     material      hypernym  information\n",
       "87340   decoration       synonym       ribbon\n",
       "53484    enclosure      hypernym         area\n",
       "65599        tongs      hypernym       device\n",
       "53967      fighter      hypernym     airplane\n",
       "194365        fall      hypernym       travel"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.sample(n=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subject relation   object\n",
      "98        away  synonym  outside\n",
      "236      plain  synonym     bare\n",
      "237      plain  synonym    spare\n",
      "414       full  synonym     good\n",
      "416       wide  synonym     full\n",
      "423       bare  synonym    spare\n",
      "507  following  synonym     next\n",
      "520       back  synonym     hind\n",
      "701        big  synonym    great\n",
      "702        big  synonym    large\n",
      "703        big  synonym    heavy\n",
      "890      hairy  synonym   haired\n"
     ]
    }
   ],
   "source": [
    "print(final_df[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GRAVEYARD\n",
    "\n",
    "# Define a function to get coordinate terms\n",
    "def get_coordinate_terms(synset):\n",
    "    coordinate_terms = set()\n",
    "    for hypernym in synset.hypernyms():  # Get the immediate hypernyms\n",
    "        # Add all hyponyms of the hypernym\n",
    "        coordinate_terms.update(hypernym.hyponyms())\n",
    "    # Remove the original synset to keep only the coordinate terms\n",
    "    coordinate_terms.discard(synset)\n",
    "    return coordinate_terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsoftlog_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
